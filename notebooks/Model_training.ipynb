{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:14.083386Z","iopub.status.busy":"2022-09-15T09:06:14.082688Z","iopub.status.idle":"2022-09-15T09:06:15.284306Z","shell.execute_reply":"2022-09-15T09:06:15.282904Z","shell.execute_reply.started":"2022-09-15T09:06:14.083290Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB, GaussianNB\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"markdown","metadata":{},"source":["### Import Data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:15.287265Z","iopub.status.busy":"2022-09-15T09:06:15.286762Z","iopub.status.idle":"2022-09-15T09:06:15.351951Z","shell.execute_reply":"2022-09-15T09:06:15.351038Z","shell.execute_reply.started":"2022-09-15T09:06:15.287217Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>v1</th>\n","      <th>v2</th>\n","      <th>Unnamed: 2</th>\n","      <th>Unnamed: 3</th>\n","      <th>Unnamed: 4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ham</td>\n","      <td>Go until jurong point, crazy.. Available only ...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ham</td>\n","      <td>Ok lar... Joking wif u oni...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>spam</td>\n","      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ham</td>\n","      <td>U dun say so early hor... U c already then say...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ham</td>\n","      <td>Nah I don't think he goes to usf, he lives aro...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     v1                                                 v2 Unnamed: 2  \\\n","0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n","1   ham                      Ok lar... Joking wif u oni...        NaN   \n","2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n","3   ham  U dun say so early hor... U c already then say...        NaN   \n","4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n","\n","  Unnamed: 3 Unnamed: 4  \n","0        NaN        NaN  \n","1        NaN        NaN  \n","2        NaN        NaN  \n","3        NaN        NaN  \n","4        NaN        NaN  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('https://raw.githubusercontent.com/bigmlcom/python/master/data/spam.csv')\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocess"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:15.353813Z","iopub.status.busy":"2022-09-15T09:06:15.353241Z","iopub.status.idle":"2022-09-15T09:06:15.481595Z","shell.execute_reply":"2022-09-15T09:06:15.480283Z","shell.execute_reply.started":"2022-09-15T09:06:15.353777Z"},"trusted":true},"outputs":[],"source":["#remove the stop words and transform the texts into the vectorized input variables X\n","vectorizer = CountVectorizer(stop_words='english')\n","X = vectorizer.fit_transform(df[\"v2\"])\n","\n","#transform the values of the output variable into 0 and 1\n","y = df[\"v1\"].map({'spam':1,'ham':0})\n","\n","#split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"]},{"cell_type":"markdown","metadata":{},"source":["### Multinomial Naive Bayes Classifier\n","\n","The multinomial NB classifier has a hyperparameter called **`alpha`**. It is the **smoothing parameter** to avoid **zero counts** when calculating the frequencies. \n","\n","For example, if we are now classifying a new SMS with a word \"ryan\" which never exist in the spam emails within our training dataset, the **likelihood** for this word will be zero. This will casue the **overall likelihood** to be zero (because we take the product of all **individual likelihoods**) for no matter what class of output variable we have.\n","\n","Therefore, we need to add **additional counts** to each word when calculating the frequencies to avoid have a zero likelihood value. **Alpha** indicates how many **additional counts** we add."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:15.484708Z","iopub.status.busy":"2022-09-15T09:06:15.484309Z","iopub.status.idle":"2022-09-15T09:06:21.434796Z","shell.execute_reply":"2022-09-15T09:06:21.433616Z","shell.execute_reply.started":"2022-09-15T09:06:15.484671Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>alpha</th>\n","      <th>accuracy</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.01</td>\n","      <td>0.980263</td>\n","      <td>0.953782</td>\n","      <td>0.911647</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.02</td>\n","      <td>0.980263</td>\n","      <td>0.962185</td>\n","      <td>0.905138</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.03</td>\n","      <td>0.980861</td>\n","      <td>0.962185</td>\n","      <td>0.908730</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.04</td>\n","      <td>0.980861</td>\n","      <td>0.962185</td>\n","      <td>0.908730</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.05</td>\n","      <td>0.980861</td>\n","      <td>0.966387</td>\n","      <td>0.905512</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   alpha  accuracy    recall  precision\n","0   0.01  0.980263  0.953782   0.911647\n","1   0.02  0.980263  0.962185   0.905138\n","2   0.03  0.980861  0.962185   0.908730\n","3   0.04  0.980861  0.962185   0.908730\n","4   0.05  0.980861  0.966387   0.905512"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#train and evaluate models with different alpha values\n","alpha_values = np.arange(0.01, 10, 0.01)\n","accuracy_scores, recall_scores, precision_scores = [], [], []\n","\n","for alpha in alpha_values:\n","    NB = MultinomialNB(alpha=alpha)\n","    NB.fit(X_train, y_train)\n","    accuracy_scores.append(metrics.accuracy_score(y_test, NB.predict(X_test)))\n","    recall_scores.append(metrics.recall_score(y_test, NB.predict(X_test)))\n","    precision_scores.append(metrics.precision_score(y_test, NB.predict(X_test)))\n","\n","performance_NB = pd.DataFrame(columns=['alpha', 'accuracy', 'recall', 'precision'])\n","performance_NB['alpha'] = alpha_values\n","performance_NB['accuracy'] = accuracy_scores\n","performance_NB['recall'] = recall_scores\n","performance_NB['precision'] = precision_scores\n","performance_NB.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:21.436331Z","iopub.status.busy":"2022-09-15T09:06:21.435997Z","iopub.status.idle":"2022-09-15T09:06:21.446441Z","shell.execute_reply":"2022-09-15T09:06:21.445097Z","shell.execute_reply.started":"2022-09-15T09:06:21.436300Z"},"trusted":true},"outputs":[{"data":{"text/plain":["alpha        3.800000\n","accuracy     0.983852\n","recall       0.911765\n","precision    0.973094\n","Name: 379, dtype: float64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["#finds the best alpha value\n","best_index = performance_NB['accuracy'].idxmax()\n","performance_NB.iloc[best_index, :]"]},{"cell_type":"markdown","metadata":{},"source":["## <a id=\"4\"></a>\n","\n","# <center>Example: Titanic Survival Prediction</center>"]},{"cell_type":"markdown","metadata":{},"source":["### Import Data"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:21.448955Z","iopub.status.busy":"2022-09-15T09:06:21.447936Z","iopub.status.idle":"2022-09-15T09:06:21.487662Z","shell.execute_reply":"2022-09-15T09:06:21.486754Z","shell.execute_reply.started":"2022-09-15T09:06:21.448905Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv('https://github.com/pcsanwald/kaggle-titanic/blob/master/train.csv')\n","df_test = pd.read_csv('https://raw.githubusercontent.com/pcsanwald/kaggle-titanic/master/test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["### Data Preprocess"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:21.489424Z","iopub.status.busy":"2022-09-15T09:06:21.489089Z","iopub.status.idle":"2022-09-15T09:06:21.541460Z","shell.execute_reply":"2022-09-15T09:06:21.540011Z","shell.execute_reply.started":"2022-09-15T09:06:21.489392Z"},"trusted":true},"outputs":[],"source":["#missing values\n","df_train = df_train.dropna(subset=['Embarked'])\n","df_train['Age'].fillna(round(df_train['Age'].dropna().mean()), inplace=True)\n","df_test['Age'].fillna(round(df_test['Age'].dropna().mean()), inplace=True)\n","\n","#feature engineering: family size\n","df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1\n","df_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1\n","df_train['Alone'] = df_train['FamilySize'].map({1:1})\n","df_train['Alone'].fillna(0, inplace=True)\n","df_test['Alone'] = df_test['FamilySize'].map({1:1})\n","df_test['Alone'].fillna(0, inplace=True)\n","\n","#feature engineering: age band\n","bins = [0, 15, 30, 60, 81]\n","labels = [0, 1, 2, 3]\n","df_train['AgeBand'] = pd.cut(df_train['Age'], bins=bins, labels=labels, right=False)\n","df_test['AgeBand'] = pd.cut(df_test['Age'], bins=bins, labels=labels, right=False)\n","\n","#feature engineering: fare band\n","bins = [0, 8, 14, 31, 513]\n","labels = [0, 1, 2, 3]\n","df_train['FareBand'] = pd.cut(df_train['Fare'], bins=bins, labels=labels, right=False)\n","df_test['FareBand'] = pd.cut(df_test['Fare'], bins=bins, labels=labels, right=False)\n","\n","#encode categorical variables\n","df_train['Sex'] = df_train['Sex'].map({'male':1, 'female':0})\n","df_test['Sex'] = df_test['Sex'].map({'male':1, 'female':0})\n","df_train = pd.get_dummies(df_train, columns= ['Embarked'])\n","df_test = pd.get_dummies(df_test, columns= ['Embarked'])\n","df_train.drop(columns=['Embarked_S'], inplace=True)\n","df_test.drop(columns=['Embarked_S'], inplace=True)\n","\n","#create datasets for modelling\n","X_train = df_train.drop(columns=['Survived','Name','Age','SibSp','Parch','Fare','PassengerId','Ticket','Cabin'])\n","y_train = df_train['Survived']\n","X_test = df_test.drop(columns=['Name','Age','SibSp','Parch','Fare','Ticket','Cabin']).copy()\n","\n","#create the validation set\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"]},{"cell_type":"markdown","metadata":{},"source":["### Gaussian Naive Bayes Classifier\n","\n","There is one hyperparameter we need to tune: **`var_smoothing`**. This is the **portion of the largest variance** of all features that is added to variances for **calculation stability**."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:21.544109Z","iopub.status.busy":"2022-09-15T09:06:21.543160Z","iopub.status.idle":"2022-09-15T09:06:31.085581Z","shell.execute_reply":"2022-09-15T09:06:31.084319Z","shell.execute_reply.started":"2022-09-15T09:06:21.544062Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>smooth</th>\n","      <th>accuracy</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.000000e-09</td>\n","      <td>0.737828</td>\n","      <td>0.7</td>\n","      <td>0.675439</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.001101e-05</td>\n","      <td>0.737828</td>\n","      <td>0.7</td>\n","      <td>0.675439</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.002102e-05</td>\n","      <td>0.737828</td>\n","      <td>0.7</td>\n","      <td>0.675439</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.003103e-05</td>\n","      <td>0.737828</td>\n","      <td>0.7</td>\n","      <td>0.675439</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.004104e-05</td>\n","      <td>0.737828</td>\n","      <td>0.7</td>\n","      <td>0.675439</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         smooth  accuracy  recall  precision\n","0  1.000000e-09  0.737828     0.7   0.675439\n","1  1.001101e-05  0.737828     0.7   0.675439\n","2  2.002102e-05  0.737828     0.7   0.675439\n","3  3.003103e-05  0.737828     0.7   0.675439\n","4  4.004104e-05  0.737828     0.7   0.675439"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#train models with different smoothing values\n","smooth_values = np.linspace(1e-9, 1e-2, 1000)\n","accuracy_scores, recall_scores, precision_scores = [], [], []\n","\n","for smooth in smooth_values:\n","    GNB = GaussianNB(var_smoothing=smooth)\n","    GNB.fit(X_train, y_train)\n","    accuracy_scores.append(metrics.accuracy_score(y_valid, GNB.predict(X_valid)))\n","    recall_scores.append(metrics.recall_score(y_valid, GNB.predict(X_valid)))\n","    precision_scores.append(metrics.precision_score(y_valid, GNB.predict(X_valid)))\n","\n","performance_GNB = pd.DataFrame(columns=['smooth', 'accuracy', 'recall', 'precision'])\n","performance_GNB['smooth'] = smooth_values\n","performance_GNB['accuracy'] = accuracy_scores\n","performance_GNB['recall'] = recall_scores\n","performance_GNB['precision'] = precision_scores\n","performance_GNB.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-09-15T09:06:31.087616Z","iopub.status.busy":"2022-09-15T09:06:31.087228Z","iopub.status.idle":"2022-09-15T09:06:31.096838Z","shell.execute_reply":"2022-09-15T09:06:31.095614Z","shell.execute_reply.started":"2022-09-15T09:06:31.087573Z"},"trusted":true},"outputs":[{"data":{"text/plain":["smooth       0.006096\n","accuracy     0.752809\n","recall       0.700000\n","precision    0.700000\n","Name: 609, dtype: float64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#finds the best smoothing value\n","best_index = performance_GNB['accuracy'].idxmax()\n","performance_GNB.iloc[best_index, :]"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 (conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"6a8f3a730b2b5112d3d12c245cbdcd746021ef8913311993047d23fe00c693bf"}}},"nbformat":4,"nbformat_minor":4}
